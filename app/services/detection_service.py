# app/services/detection_service.py
import os
import yaml
import cv2
import torch
import torchvision.ops as ops
import numpy as np
import multiprocessing
from tqdm import tqdm
from typing import List, Tuple
import time # âœ¨ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ time ëª¨ë“ˆ import

# ì„œë¹„ìŠ¤ ë° í—¬í¼ í•¨ìˆ˜ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ import
from .pdf_service import convert_pdf_to_images
from .ocr_service import perform_ocr_on_crops_and_export


# --- í—¬í¼ í•¨ìˆ˜ë“¤ ---

def load_config(config_path: str) -> dict:
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_model(model_path, device='cpu'):
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ -> {model_path}")
    model = torch.load(model_path, map_location=device, weights_only=False)
    model.eval().to(device)
    return model


# ... (sliding_window_coords, detect_objects_in_tiles, annotate_image, crop_and_save_objects í•¨ìˆ˜ëŠ” ì—¬ê¸°ì— ê·¸ëŒ€ë¡œ ë³µì‚¬) ...

def sliding_window_coords(h: int, w: int, tile_size: int, overlap: int) -> List[Tuple[int, int, int, int]]:
    coords = []
    step = tile_size - overlap
    for y in range(0, h, step):
        for x in range(0, w, step):
            coords.append((x, y, min(x + tile_size, w), min(y + tile_size, h)))
    return coords


def detect_objects_in_tiles(model, full_image, tiles_coords, device, threshold, batch_size=8):
    all_detections = []
    tiles_with_offsets = [(full_image[y1:y2, x1:x2], (x1, y1)) for x1, y1, x2, y2 in tiles_coords]
    for i in range(0, len(tiles_with_offsets), batch_size):
        batch = tiles_with_offsets[i:i + batch_size]
        imgs_tensor = []
        for img, _ in batch:
            if img.shape[0] == 0 or img.shape[1] == 0: continue
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img_chw = np.transpose(img_rgb, (2, 0, 1))
            tensor = torch.from_numpy(np.ascontiguousarray(img_chw)).float().div(255).to(device)
            imgs_tensor.append(tensor)
        if not imgs_tensor: continue
        with torch.no_grad():
            preds = model(imgs_tensor)
        for (img_arr, (x_off, y_off)), pred in zip(batch, preds):
            for box, label, score in zip(pred['boxes'], pred['labels'], pred['scores']):
                if score.item() >= threshold:
                    x1, y1, x2, y2 = map(int, box.cpu().numpy())
                    all_detections.append((x1 + x_off, y1 + y_off, x2 + x_off, y2 + y_off, score.item(), label.item()))
    return all_detections


def annotate_image(image_path: str, detections: list, class_names: list, output_path: str):
    image = cv2.imread(image_path)
    for x1, y1, x2, y2, score, label_id in detections:
        label, text = class_names[int(label_id)], f"{class_names[int(label_id)]}: {score:.2f}"
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(image, (x1, y1 - h - 10), (x1 + w, y1), (0, 255, 0), -1)
        cv2.putText(image, text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    cv2.imwrite(output_path, image)


def crop_and_save_objects(full_image, detections: list, class_names: list, output_dir: str, original_image_name: str):
    crop_output_dir = os.path.join(output_dir, "3_cropped_objects")
    os.makedirs(crop_output_dir, exist_ok=True)
    base_name = os.path.splitext(original_image_name)[0]
    for i, (x1, y1, x2, y2, score, label_id) in enumerate(detections):
        class_name = class_names[int(label_id)]
        cropped_img = full_image[y1:y2, x1:x2]
        crop_filename = f"{base_name}_{i:03d}_{class_name}_{score:.2f}.png"
        crop_path = os.path.join(crop_output_dir, crop_filename)
        if cropped_img.shape[0] > 0 and cropped_img.shape[1] > 0:
            cv2.imwrite(crop_path, cropped_img)


# --- ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ---

def init_worker(model_path, device_str):
    global worker_model, worker_device
    worker_device = torch.device(device_str)
    worker_model = load_model(model_path, worker_device)
    print(f"í”„ë¡œì„¸ìŠ¤ {os.getpid()}: ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")


def process_single_image(args, model, device):
    image_path, config, output_dir, tile_size, overlap, iou_thresh, conf_thresh, target_label_ids = args
    # global worker_model, worker_device # âœ¨ ë” ì´ìƒ ì „ì—­ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‚­ì œ

    annotated_images_dir = os.path.join(output_dir, "2_annotated_images")
    full_image = cv2.imread(image_path)
    if full_image is None: return f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}"

    h, w, _ = full_image.shape
    tiles_coords = sliding_window_coords(h, w, tile_size, overlap)

    # âœ¨ ì „ì—­ ëª¨ë¸ ëŒ€ì‹  ì „ë‹¬ë°›ì€ ëª¨ë¸ê³¼ ë””ë°”ì´ìŠ¤ ì‚¬ìš©
    raw_detections = detect_objects_in_tiles(model, full_image, tiles_coords, device, conf_thresh)
    if not raw_detections: return f"ê°ì²´ ë¯¸íƒì§€: {os.path.basename(image_path)}"
    target_detections = [d for d in raw_detections if d[5] in target_label_ids]
    if not target_detections: return f"íƒ€ê²Ÿ ê°ì²´ ë¯¸íƒì§€: {os.path.basename(image_path)}"
    boxes_tensor = torch.tensor([d[:4] for d in target_detections], dtype=torch.float32)
    scores_tensor = torch.tensor([d[4] for d in target_detections], dtype=torch.float32)
    keep_indices = ops.nms(boxes_tensor, scores_tensor, iou_thresh).cpu().numpy()
    final_detections = [target_detections[i] for i in keep_indices]
    output_image_path = os.path.join(annotated_images_dir, os.path.basename(image_path))
    annotate_image(image_path, final_detections, config['class_names'], output_image_path)
    crop_and_save_objects(full_image, final_detections, config['class_names'], output_dir, os.path.basename(image_path))
    return f"ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(image_path)} ({len(final_detections)}ê°œ íƒì§€)"


# --- ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ---

def run_analysis_pipeline(job_id, pdf_path, settings, output_dir, update_job_status_func,
                          config_path="config/config.yaml"):
    total_start_time = time.time()  # ì „ì²´ ì‹œì‘ ì‹œê°„ ê¸°ë¡

    config = load_config(config_path)
    class_names = config.get('class_names', [])
    target_label_ids = {class_names.index(label) for label in settings['targetLabels'] if label in class_names}

    # --- ë‹¨ê³„ 1: PDF ë³€í™˜ ---
    step1_start_time = time.time()
    update_job_status_func(job_id, 'running', 10, 'PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘...')
    source_images_dir = os.path.join(output_dir, "1_source_images")
    image_paths = convert_pdf_to_images(pdf_path, source_images_dir, config.get('pdf_render_dpi', 300))
    print(f"â±ï¸  PDF ë³€í™˜ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - step1_start_time:.2f}ì´ˆ")

    # --- ë‹¨ê³„ 2: ê°ì²´ íƒì§€ (ë³‘ë ¬ ì²˜ë¦¬ ì œê±°) ---
    step2_start_time = time.time()
    update_job_status_func(job_id, 'running', 30, f'{len(image_paths)}ê°œ í˜ì´ì§€ì— ëŒ€í•œ ê°ì²´ íƒì§€ ì‹œì‘...')

    # âœ¨ ëª¨ë¸ì„ ì—¬ê¸°ì„œ ë”± í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
    device_str = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device_str)
    model = load_model(config['model_path'], device)
    print(f"ğŸ§  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ({device_str} ì‚¬ìš©)")

    TILE_SIZE = 1024
    OVERLAP = 200
    IOU_THRESHOLD = 0.5
    CONFIDENCE_THRESHOLD = 0.4

    task_args = [(path, config, output_dir, TILE_SIZE, OVERLAP, IOU_THRESHOLD, CONFIDENCE_THRESHOLD, target_label_ids)
                 for path in image_paths]

    # âœ¨ ë³‘ë ¬ ì²˜ë¦¬ Pool ëŒ€ì‹  ê°„ë‹¨í•œ for ë°˜ë³µë¬¸ìœ¼ë¡œ ë³€ê²½!
    print("â¡ï¸  ìˆœì°¨ì  ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘...")
    for i, args in enumerate(task_args):
        # âœ¨ ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ ë°©ì‹
        result_msg = process_single_image(args, model, device)
        progress = 30 + int(60 * (i + 1) / len(image_paths))
        update_job_status_func(job_id, 'running', progress, f'í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ({i + 1}/{len(image_paths)})...')
        print(f"    - {result_msg}")
    print(f"â±ï¸  ëª¨ë“  ì´ë¯¸ì§€ ê°ì²´ íƒì§€ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - step2_start_time:.2f}ì´ˆ")

    # --- ë‹¨ê³„ 3: OCR ë° ì—‘ì…€ ì¶”ì¶œ ---
    step3_start_time = time.time()
    update_job_status_func(job_id, 'running', 90, 'ê°ì²´ íƒì§€ ì™„ë£Œ, OCR ë° ì—‘ì…€ ì¶”ì¶œ ì‹œì‘...')
    cropped_dir = os.path.join(output_dir, "3_cropped_objects")
    excel_path = os.path.join(output_dir, "ocr_and_detection_results.xlsx")

    perform_ocr_on_crops_and_export(
        cropped_images_dir=cropped_dir,
        output_excel_path=excel_path,
        pdf_path=pdf_path,
        reference_text=settings['referenceText'],
        reference_page=settings['referencePage']
    )
    print(f"â±ï¸  OCR ë° ì—‘ì…€ ì €ì¥ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - step3_start_time:.2f}ì´ˆ")

    annotated_dir = os.path.join(output_dir, '2_annotated_images')
    total_symbols = len(os.listdir(cropped_dir)) if os.path.exists(cropped_dir) else 0

    print(f"\nâœ¨ ì´ ì†Œìš” ì‹œê°„: {time.time() - total_start_time:.2f}ì´ˆ")
    return {
        'success': True,
        'total_pages': len(image_paths),
        'total_symbols': total_symbols,
        'excel_path': excel_path,
        'annotated_images_dir': annotated_dir
    }