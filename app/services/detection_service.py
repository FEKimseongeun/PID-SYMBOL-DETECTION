# app/services/detection_service.py
import os
import yaml
import cv2
import torch
import torchvision.ops as ops
import numpy as np
from tqdm import tqdm
from typing import List, Tuple
import time

# ì„œë¹„ìŠ¤ ë° í—¬í¼ í•¨ìˆ˜ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ import
from .pdf_service import convert_pdf_to_images
from .ocr_service import perform_ocr_on_detections_and_export


# --- í—¬í¼ í•¨ìˆ˜ë“¤ ---

def load_config(config_path: str) -> dict:
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_model(model_path, device='cpu'):
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ -> {model_path}")
    model = torch.load(model_path, map_location=device, weights_only=False)
    model.eval().to(device)
    return model


def sliding_window_coords(h: int, w: int, tile_size: int, overlap: int) -> List[Tuple[int, int, int, int]]:
    coords = []
    step = tile_size - overlap
    for y in range(0, h, step):
        for x in range(0, w, step):
            coords.append((x, y, min(x + tile_size, w), min(y + tile_size, h)))
    return coords


def detect_objects_in_tiles(model, full_image, tiles_coords, device, threshold, batch_size=8):
    all_detections = []
    tiles_with_offsets = [(full_image[y1:y2, x1:x2], (x1, y1)) for x1, y1, x2, y2 in tiles_coords]
    for i in range(0, len(tiles_with_offsets), batch_size):
        batch = tiles_with_offsets[i:i + batch_size]
        imgs_tensor = []
        for img, _ in batch:
            if img.shape[0] == 0 or img.shape[1] == 0: continue
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img_chw = np.transpose(img_rgb, (2, 0, 1))
            tensor = torch.from_numpy(np.ascontiguousarray(img_chw)).float().div(255).to(device)
            imgs_tensor.append(tensor)
        if not imgs_tensor: continue
        with torch.no_grad():
            preds = model(imgs_tensor)
        for (img_arr, (x_off, y_off)), pred in zip(batch, preds):
            for box, label, score in zip(pred['boxes'], pred['labels'], pred['scores']):
                if score.item() >= threshold:
                    x1, y1, x2, y2 = map(int, box.cpu().numpy())
                    all_detections.append((x1 + x_off, y1 + y_off, x2 + x_off, y2 + y_off, score.item(), label.item()))
    return all_detections


def annotate_image(image_path: str, detections: list, class_names: list, output_path: str):
    """ì´ë¯¸ì§€ì— íƒì§€ ê²°ê³¼ë¥¼ ì‹œê°í™”í•´ì„œ ì €ì¥"""
    image = cv2.imread(image_path)
    for x1, y1, x2, y2, score, label_id in detections:
        label, text = class_names[int(label_id)], f"{class_names[int(label_id)]}: {score:.2f}"
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(image, (x1, y1 - h - 10), (x1 + w, y1), (0, 255, 0), -1)
        cv2.putText(image, text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    cv2.imwrite(output_path, image)


def process_single_image(args, model, device):
    """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê°ì²´ íƒì§€ ìˆ˜í–‰"""
    image_path, config, output_dir, tile_size, overlap, iou_thresh, conf_thresh, target_label_ids = args

    annotated_images_dir = os.path.join(output_dir, "2_annotated_images")
    full_image = cv2.imread(image_path)
    if full_image is None:
        return {"success": False, "message": f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}", "detections": []}

    h, w, _ = full_image.shape
    tiles_coords = sliding_window_coords(h, w, tile_size, overlap)

    # ê°ì²´ íƒì§€ ìˆ˜í–‰
    raw_detections = detect_objects_in_tiles(model, full_image, tiles_coords, device, conf_thresh)
    if not raw_detections:
        return {"success": False, "message": f"ê°ì²´ ë¯¸íƒì§€: {os.path.basename(image_path)}", "detections": []}

    # íƒ€ê²Ÿ ë¼ë²¨ë§Œ í•„í„°ë§
    target_detections = [d for d in raw_detections if d[5] in target_label_ids]
    if not target_detections:
        return {"success": False, "message": f"íƒ€ê²Ÿ ê°ì²´ ë¯¸íƒì§€: {os.path.basename(image_path)}", "detections": []}

    # NMS ì ìš©
    boxes_tensor = torch.tensor([d[:4] for d in target_detections], dtype=torch.float32)
    scores_tensor = torch.tensor([d[4] for d in target_detections], dtype=torch.float32)
    keep_indices = ops.nms(boxes_tensor, scores_tensor, iou_thresh).cpu().numpy()
    final_detections = [target_detections[i] for i in keep_indices]

    # ì–´ë…¸í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ì €ì¥ (ì‹œê°í™”ìš©)
    output_image_path = os.path.join(annotated_images_dir, os.path.basename(image_path))
    annotate_image(image_path, final_detections, config['class_names'], output_image_path)

    return {
        "success": True,
        "message": f"ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(image_path)} ({len(final_detections)}ê°œ íƒì§€)",
        "detections": final_detections,
        "image_shape": (h, w, full_image.shape[2]),
        "image_path": image_path
    }


# --- ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ---

def run_analysis_pipeline(job_id, pdf_path, settings, output_dir, update_job_status_func,
                          config_path="config/config.yaml"):
    total_start_time = time.time()

    config = load_config(config_path)
    class_names = config.get('class_names', [])
    target_label_ids = {class_names.index(label) for label in settings['targetLabels'] if label in class_names}

    # --- ë‹¨ê³„ 1: PDF ë³€í™˜ ---
    step1_start_time = time.time()
    update_job_status_func(job_id, 'running', 10, 'PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘...')
    source_images_dir = os.path.join(output_dir, "1_source_images")
    image_paths = convert_pdf_to_images(pdf_path, source_images_dir, config.get('pdf_render_dpi', 300))
    print(f"â±ï¸  PDF ë³€í™˜ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - step1_start_time:.2f}ì´ˆ")

    # --- ë‹¨ê³„ 2: ê°ì²´ íƒì§€ ---
    step2_start_time = time.time()
    update_job_status_func(job_id, 'running', 30, f'{len(image_paths)}ê°œ í˜ì´ì§€ì— ëŒ€í•œ ê°ì²´ íƒì§€ ì‹œì‘...')

    # ëª¨ë¸ ë¡œë“œ
    device_str = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device_str)
    model = load_model(config['model_path'], device)
    print(f"ğŸ§  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ({device_str} ì‚¬ìš©)")

    # íƒì§€ íŒŒë¼ë¯¸í„°
    TILE_SIZE = 1024
    OVERLAP = 200
    IOU_THRESHOLD = 0.5
    CONFIDENCE_THRESHOLD = 0.4

    task_args = [(path, config, output_dir, TILE_SIZE, OVERLAP, IOU_THRESHOLD, CONFIDENCE_THRESHOLD, target_label_ids)
                 for path in image_paths]

    # ëª¨ë“  í˜ì´ì§€ì˜ íƒì§€ ê²°ê³¼ë¥¼ ì €ì¥
    all_detection_results = []

    print("â¡ï¸  ìˆœì°¨ì  ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘...")
    for i, args in enumerate(task_args):
        result = process_single_image(args, model, device)
        all_detection_results.append(result)
        progress = 30 + int(50 * (i + 1) / len(image_paths))  # 30~80% í• ë‹¹
        update_job_status_func(job_id, 'running', progress, f'í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ({i + 1}/{len(image_paths)})...')
        print(f"    - {result['message']}")

    print(f"â±ï¸  ëª¨ë“  ì´ë¯¸ì§€ ê°ì²´ íƒì§€ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - step2_start_time:.2f}ì´ˆ")

    # --- ë‹¨ê³„ 3: OCR ë° ì—‘ì…€ ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹) ---
    step3_start_time = time.time()
    update_job_status_func(job_id, 'running', 85, 'íƒì§€ëœ ê°ì²´ì— ëŒ€í•œ OCR ì²˜ë¦¬ ì‹œì‘...')

    excel_path = os.path.join(output_dir, "ocr_and_detection_results.xlsx")

    # âœ¨ ìƒˆë¡œìš´ OCR í•¨ìˆ˜ í˜¸ì¶œ - íƒì§€ ê²°ê³¼ë¥¼ ì§ì ‘ ì „ë‹¬
    perform_ocr_on_detections_and_export(
        detection_results=all_detection_results,
        output_excel_path=excel_path,
        pdf_path=pdf_path,
        reference_text=settings['referenceText'],
        reference_page=settings['referencePage'],
        class_names=class_names
    )

    print(f"â±ï¸  OCR ë° ì—‘ì…€ ì €ì¥ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - step3_start_time:.2f}ì´ˆ")

    annotated_dir = os.path.join(output_dir, '2_annotated_images')

    # ì´ íƒì§€ëœ ì‹¬ë³¼ ìˆ˜ ê³„ì‚°
    total_symbols = sum(len(result.get('detections', [])) for result in all_detection_results if result['success'])

    print(f"\nâœ¨ ì´ ì†Œìš” ì‹œê°„: {time.time() - total_start_time:.2f}ì´ˆ")
    return {
        'success': True,
        'total_pages': len(image_paths),
        'total_symbols': total_symbols,
        'excel_path': excel_path,
        'annotated_images_dir': annotated_dir
    }